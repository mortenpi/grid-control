; ==============================================================================
; General options
; ==============================================================================

[DEFAULT]
dir = .

[global]
module        = CMSSW         ; Available options: 
                              ; CMSSW, UserMod, SimpleParaMod, FileParaMod, LinkedParaMod
                              ; |ADVANCED USAGE: Classes can be specified in different ways:
                              ; | * grid_control.user_mod.UserMod (fully qualified path)
                              ; | * user_mod.UserMod (lookup in grid_control is default)
                              ; | * UserMod (short form in case of import by __init__.py)
backend       = grid          ; Available options: [grid], local
workdir       = %(dir)s/work  ; Location of the work directory - default: Name of config file
workdir space = 50            ; Lower space limit in work directory, deactivate with 0 - default: 10
include       = common.conf   ; This config files provides default values
cmdargs       = -G -c         ; Automatically added command line arguments - default: empty
                              ; Here, -G -c enables the GUI and continuous mode

[jobs]
jobs          = 27            ; Maximum number of jobs (truncated to module maximum)
                              ; Default is taken from module maximum
in flight     = 10            ; Maximum number of concurrently submitted jobs - default: no limit
in queue      = -1            ; Maximum number of queued jobs - default: No limit
max retry     = 4             ; Number of resubmission attempts for failed jobs - default: no limit
continuous    = True          ; Enable continuous mode - default: False

cpus          = 1             ; Requested number of cpus per node - default: 1
memory        = 512           ; Requested memory in MB - default: 512
                              ; NAF jobs need 2000 MB !
wall time     = 10:00:00      ; Requested wall time in format hh[:mm[:ss]]
                              ; also used for checking the proxy lifetime
cpu time      = 10:00         ; Requested cpu time in format hh[:mm[:ss]] - default: wall time

queue timeout = 2:00:00       ; Resubmit jobs after staying some time in initial state - default: off
;node timeout = 0:10:00       ; Cancel job after some time on worker node - default: off
monitor       = dashboard     ; Monitoring of jobs (can be combined like: "dashboard,scripts")
                              ;    dashboard - use dashboard
                              ;    scripts   - call scripts (see [events] section) [default]
shuffle       = True          ; Submit jobs in random order - default: False
seeds         = 32,51,346,234 ; Random seeds used in the job via @SEED_j@
                              ; @SEED_0@ = 32, 33, 34, ... for first, second, third job
                              ; @SEED_1@ = 51, 52, 53, ... for first, second, third job
                              ; Default: Generate 10 random seeds

[events]
silent        = True          ; Do not show output of event scripts - default: true
; There are many variables set for these scripts according to the
; environment variables the job will have during execution - they are prefixed "GC_"
; note: the events are only evaluated with "monitor = scripts" in [jobs] section
; In addition to the normal job variables, the following variables are always defined:
;   CFGFILE = absolute path to config file
;   WORKDIR = absolute path to working directory
on submit     = on_submit.sh @CFGFILE@ @JOBNUM@
; To next event has access to the variable "STATUS":
on status     = on_status.sh @NICK@ @STATUS@
; The following event has the additional variable RETCODE (= return code of job)
on output     = on_output.sh @WORKDIR@ @RETCODE@
; For the following event only job invariant constants and the
; variable NJOBS (= number of jobs) are available
on finish     = on_finish.sh @TASK_ID@ @NJOBS@

[dashboard]
application   = Herwigpp      ; Defines application name - default: shellscript
task          = production    ; Task type reported to the dashboard - default: analysis
task name     = @TASK_ID@     ; Taskname in dashboard - default: @TASK_ID@_@NICK@
                              ; ("_" are stripped away from the edges)

; ==============================================================================
; Backend options
; ==============================================================================

[local]
proxy         = TrivialProxy  ; Available options: [TrivialProxy], VomsProxy
check storage = False         ; Check storage requirements before submission - default: True
                              ;   False: Submit jobs even with empty storage requirements
wms           = PBS           ; Select local wms: PBS, LSF, SGE, SLURM, Host - default: best guess
sites         = -node003      ; Whitelist / Blacklist nodes (prefix "-")
queue         = short         ; Select local queue
broker        = DummyBroker   ; Available options: [DummyBroker], SimpleBroker

sandbox path  = %(dir)s/sbox  ; Path to sandboxes - default: $WORKDIR/sandbox
scratch path  = /tmp          ; Override scratch directory on nodes - default: determined by job
server        = servername    ; Name of batch server - default: None
group         = cmsqcd        ; Select local fairshare group

[grid]
proxy         = VomsProxy     ; Available options: TrivialProxy, [VomsProxy]
wms           = GliteWMS      ; Available options: [GliteWMS], Glite, LCG
sites         = -ce.grid.net  ; Whitelist / Blacklist sites (prefix "-")
vo            = cms           ; Select vo - default: provided by proxy

[proxy]
ignore warnings   = True      ; Ignore problems with the proxy as long as all
                              ; necessary information is provided - default: False

[glite-wms]
ce            = ce1.gridka.de ; Select destination CE
;config = docs/glite_wms_ALL.conf       ; GliteWMS backend specific configuration (WMS, ...)

; ==============================================================================
; Storage options
; ==============================================================================

[storage]
; se path specifies the location used to transfer "se input files" at the beginning of the job
; and "se output files" at the end of the job
; Currently supported protocols: gsiftp srm dir
se path       = gsiftp://ekp-lcg-se.physik.uni-karlsruhe.de//wlcg/data/users/cms/my_username
;se path      = srm://dcache-se-cms.desy.de:8443/pnfs/desy.de/cms/tier2/store/user/
;se path      = srm://dcache-se-cms.desy.de:8443//srm/managerv2?SFN=/pnfs/desy.de/cms/tier2/store/user/
;se path      = rfio:///castor/cern.ch/user/x/username
;se path      = dir:///absolute/path/to/directory

se min size       = -1                  ; Job fails if any output file is smaller than se min size
se output files   = out.root            ; Specifies the files to be transfered after the job has finished
se output pattern = job_@MY_JOBID@_@X@  ; This pattern is applied to the se output file
                                        ; to get the destination filename
                                        ; Default: @NICK@job_@MY_JOBID@_@X@
                                        ; @X@         : Marks the spot of the original filename
                                        ; @XBASE@     : Original filename without extension
                                        ; @XEXT@      : Only the extension of the original filename
                                        ; @MY_JOBID@  : Continous job number 0..max
                                        ; @NICK@      : Nickname of dataset for CMSSW jobs
                                        ; @CONF@      : Name of config file (without extension .conf)
                                        ; @DATE@      : Current date eg. 2009-06-24
                                        ; @TIMESTAMP@ : Current timestamp eg. 1245855548
                                        ; @RANDOM@    : Random number between 0 and 900000000
                                        ; This is just the list of the most important substituions available
                                        ; A complete list is available via the --help-vars option
                                        ; The variables can also be surounded by "__" instead of "@"

se input files    = file                ; Specifies the files to be transfered before the job starts
se input pattern  = @X@                 ; This pattern is applied to the se input file
                                        ; to get the source filename. Same rules as output pattern
                                        ; Default: @X@

; During the duration of the job both the available and used space is monitored
; The following entries specify thresholds (in mb) which cause the job to abort
; Landing zone is the directory the job initially starts in
landing zone space used = 100           ; Maximum amount of space used by the job
                                        ; Default: 100 mb
landing zone space left = 50            ; Minimum amout of disk space available
                                        ; Default: 1 mb
; One of the first orders of business for each job is to find a large
; scratch space which will be used as working directory of the job
; If the landing zone ITSELF is the scratch space, the scratch thresholds apply
scratch space used = 5000               ; Maximum amount of space used by the job
                                        ; Default: 5000 mb
scratch space left = 1000               ; Minimum amout of disk space available
                                        ; Default: 1 mb

; ==============================================================================
; User Module
; ==============================================================================

[UserMod]
send executable = False                 ; Put executable into the input sandbox - default: True
executable   = default.sh               ; Name of the script / application
arguments    = param1 param2 param3     ; Parameters for the user application
                                        ; Known variables in the form @VAR@ will be replaced.
                                        ; A complete list is available via the --help-vars option
input files  = input.txt config.sh      ; Input files send together with the job
                                        ; Only for small files - send large files via SE!
subst files  = config.sh                ; These input files will be subjected to variable substituion
                                        ; A complete list is available via the --help-vars option
output files = output.gz                ; Output files retrived together with the job
                                        ; Only for small files - send large files via SE!
depends      = CMSSW                    ; Depends on certain environments eg. Herwig++ job which needs
                                        ; CMSSW. Possible values: CMSSW, gLite
                                        ; The corresponding files are share/env.cmssw.sh, share/env.glite.sh
constants    = CONST1 CONST2            ; Define additional constants used in variable substitution:
CONST1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

; ==============================================================================
; CMSSW Module
; ==============================================================================

[CMSSW]
project area     = %(dir)s/CMSSW_3_1_0  ; Path to an existing CMSSW project area used for running the jobs
;scram project   = CMSSW CMSSW_3_1_0    ; Used to create a vanilla CMSSW project, eg. for production
;scram arch       = slc4_ia32_gcc345    ; Select scram architecture
                                        ; When given a project area, the default arch is taken from the project
                                        ; Has to be specified when using scram project
cmssw dir        = /path/for/VO_CMS_SW_DIR ; Path to manually add cmssw dir to search path

; Path to CMSSW config file 
config file      = %(project area)s/src/Test/Analysis/cmssw-grid.py
prepare config   = True                 ; Append fragment for CMSSW all config files - default: False

use requirements = True                 ; Write CMSSW version into job requirements - default: True
gzip output      = True                 ; Gzip the output of the cmsRun command - default: True
se runtime       = True                 ; Send CMSSW runtime via SE instead of sending it together with the job - default: False
se runtime force = True                 ; Force to overwrite existing se runtimes - default: True

; Comment out the variable [jobs] jobs in order to run over all events of the dataset
; Specifiy one dataset 
;dataset = /WmunuJets_pt_80_120/CMSSW_1_3_1-Spring07-1243/GEN-SIM-DIGI-RECO
; Or several by starting with an empty line
dataset =
	/WmunuJets_pt_80_120/CMSSW_1_3_1-Spring07-1243/GEN-SIM-DIGI-RECO#2c1efdb8-d9ba-46d4-b067-72c3d8b19abf
	QCD : /QCD_Pt_470_600/CMSSW_1_5_2-CSA07-2096/GEN-SIM-DIGI-RECO@cms_dbs_prod_local_09
	/CSA07JetMET/CMSSW_1_6_7-CSA07-Tier0-A1-Gumbo/RECO
	Zmm : DBS:/Zmumu/Summer08_IDEAL_V9_v1/GEN-SIM-RAW
;	Nick1 : list:/path/to/local/dbsfile
;	Nick2 : file:/pnfs/to/file|1200
; dataset syntax:
;     [<nickname> : [<protocol> :]] <dataset specifier>
;     Syntax for the dataset specifier depends on the protocol:
;          dbs : <dataset path>[@<instance>][#<block>]
;         list : <path to list of data files>[@<forced prefix>][%<selected dataset>[#<selected block>]]
;                The list provider allows to select single datasets / blocks from a file
;         file : <path to data file>|<number of events>[@SE1,SE2]

; Select default dataset protocol
; Available: [DBSApiv2] (=dbs) FileProvider (=file) ListProvider (=list)
dataset provider = DBSApiv2
dataset refresh  = 4:00       ; Refresh dataset information every 4h
dataset storage check = False ; Check storage requirements before submission - default: True
                              ;   False: Submit jobs even with empty storage requirements

; Select default dbs instance for DBSApiv2 datasets
; Default: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet (Global CMSSW production DBS server instance)
dbs instance     = http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet
dbs blacklist T1 = True ; Blacklist data on T1s

dataset splitter = EventBoundarySplitter ; Available options:
                                         ; EventBoundarySplitter: splits on block boundaries and slices each block into
                                         ;                        jobs with "events per job" events per job
                                         ;                        (Default for CMSSW jobs)
                                         ; HybridSplitter:        hybrid of EventBoundarySplitter and FileBoundarySplitter,
                                         ;                        splits on file and block boundaries and tries to use
                                         ;                        approximately "events per job" events per job
                                         ; FileBoundarySplitter:  splits on file and block boundaries
                                         ;                        and interpret "files per job" as files per job
                                         ;                        (Default for UserMod jobs)
                                         ; BlockBoundarySplitter: split along block boundaries

events per job   = 5000                  ; Set granularity of dataset splitter
files per job    = 5                     ; Set granularity of dataset splitter

lumi filter      = 132440:1-132440:401,  ; apply global luminosity block filter for all DBS datasets
                   132442-132447,        ; Syntax: <RUN>[:<LUMI>]-[<RUN>[:<LUMI>]], ...
                   132473, 132652,       ; (open ended ranges possible)
                   /path/to/json_file    ; alternative way to specify lumi sections is via json file
                   json_fileY|-12345     ; get infos from json file up to run 12345

; Select files to be included in the CMSSW runtime
; Default: -.* -config lib module */data *.xml *.sql *.cf[if] *.py
area files       = -.* -config lib module */data *.xml *.sql *.cf[if] *.py

;input files     =                      ; Input files send together with the job
                                        ; Only for small files - send large files via SE!
;subst files     =                      ; These input files will be subjected to variable substituion
                                        ; A complete list is available via the --help-vars option
                                        ; Default: the CMSSW config file
;output files    =                      ; Output files retrived together with the job
                                        ; Only for small files - send large files via SE!
constants    = CONST1 CONST2            ; Define additional constants used in variable substitution:
CONST1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

[constants]
; Define additional constants (always uppercase) to allow usage in variable substitution:
; These variables are overridden by the constants specified in the module section
const1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

[dataset]
sites               = -gridka.de        ; White/Blacklist for storage location of dataset based jobs
remove empty blocks = False             ; Remove empty blocks from processing - default: true
remove empty files  = False             ; Remove empty files from processing - default: true

limit events        = 10000             ; Stop processing after 10000 events - default: -1

resync interactive  = True              ; Switch for interactive resync mode - default: true
resync mode new     = ignore            ; Resync mode for new files - [append], ignore
resync mode removed = replace           ; Resync mode for removed files - disable, [append], replace, ignore
resync mode expand  = replace           ; Resync mode for expanded files - disable, [append], replace, ignore
resync mode shrink  = replace           ; Resync mode for shrunken files - disable, [append], replace, ignore

resync preserve     = False             ; Preserve affected jobs, which stay the same after resync - default: true
resync reorder      = True              ; Reassign job slots of disabled jobs - default: false

; ==============================================================================
; Parameter Module
;   The parameter module allows to run jobs from another module with
;   different parameters.
; ==============================================================================

[ParaMod]
module           = CMSSW                ; Available options: CMSSW, UserMod
jobs             = 4                    ; In case the selected module doesn't specify the maximum number
                                        ; of jobs, set the basic number of jobs (to be multiplied by
                                        ; parameter space size) - default: 1

; ------------------------
; SimpleParaMod
;   The following is specific to SimpleParaMod:
;   SimpleParaMod provides the job with a single parameter
; ------------------------

parameter name   = MYTESTPARAM          ; Specify name of parameter - default: PARAMETER
                                        ; Parameter is set as an environment variable
                                        ; and is useable in subst files via eg. @MYTESTPARAM@
parameter values = 23 42 123            ; Specify parameter values
                                        ; In conjunction with jobs = 4 above this results in
                                        ; 4 x 3 = 12 jobs to be submitted in case the module
                                        ; itself does not specify the maximum number of jobs.
                                        ; If the module does specify the max #jobs
                                        ; (CMSSW with dataset - eg. 6) this would give 6 x 3 = 18 jobs

; ------------------------
; LinkedParaMod
;   The following is specific to LinkedParaMod:
;   LinkedParaMod provides the job with several parameters at once
; ------------------------

parameter name   = CUTLOW:CUTHIGH:XSEC  ; Specify names of parameters - default: PARAMETER
                                        ; The parameters are set as environment variables
                                        ; and are useable in subst files via eg. @XSEC@
parameter values =                      ; Specify parameter values
    20 :  40 : 1.342                    ; In conjunction with jobs = 4 above this results in
    40 : 100 : 2.124                    ; 4 x 5 = 20 jobs to be submitted in case the module
   200 : 300 : 3.134                    ; itself does not specify the maximum number of jobs.
   300 : 600 : 1.235                    ; If the module does specify the max #jobs
   700 : 800 : 0.942                    ; (CMSSW with dataset - eg. 6) this would give 6 x 5 = 30 jobs
                                        ; The first job would get CUTLOW=20, CUTHIGH=40, XSEC=1.342
                                        ; The last job would get CUTLOW=700, CUTHIGH=800, XSEC=0.942

; ------------------------
; FileParaMod
;   The following is specific to FileParaMod:
;   FileParaMod provides the job with several parameters at once from a csv file
; ------------------------

parameter source = parameters.csv       ; Specify the file the parameters are taken from.
                                        ; The header specifies the variable names
parameter source dialect = excel-tab    ; Default is to guess the csv format from the file itself
                                        ; Available options: [sniffed], excel, excel-tab

; ------------------------
; MultiParaMod
;   builds all possible combinations of parameters and parameter tuples.
;   This module is similar to the LinkedParaMod and ParaMod itself.
; ------------------------

parameters  = spam (ham, eggs)    ; Specify a parameter and two linked
                                  ; parameters
(ham, eggs) = (A, a) (B, b)       ; Assign values to the linked parameters
spam        = range(1,3)          ; Assign values to the single parameter
spam type   = expr                ; How to process the right hand side of config options:
                                  ;    words   : Split on whitespace [default]
                                  ;    lines   : Split on newlines
                                  ;    expr    : Evaluate as python expression
                                  ;    binning : Combine input into bins
                                  ;              Example: 1 2 3 4 => (1, 2) (2, 3) (3, 4)

; This gives [spam, ham, eggs]: [0,A,a], [0,B,b], [1,A,a] and [1,B,b] as
; parameter combinations

; In more complex situations which are outside the scope of either
; SimpleParaMod, FileParaMod, LinkedParaMod or MultiParaMod: => derive own module from ParaMod (very easy)
