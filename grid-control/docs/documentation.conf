; ==============================================================================
; General options
; ==============================================================================

[DEFAULT]
dir = .

[global]
module        = CMSSW         ; Available options: 
                              ; CMSSW, UserMod, SimpleParaMod, FileParaMod, LinkedParaMod
                              ; |ADVANCED USAGE: Classes can be specified in different ways:
                              ; | * grid_control.user_mod.UserMod (fully qualified path)
                              ; | * user_mod.UserMod (lookup in grid_control is default)
                              ; | * UserMod (short form in case of import by __init__.py)
backend       = grid          ; Available options: [grid], local
workdir       = %(dir)s/work  ; Location of the work directory - default: Name of config file
workdir space = 50            ; Lower space limit in work directory, deactivate with 0 - default: 10
include       = common.conf   ; List of additional config files which provide default values.
                              ; These config files are processed in addition to the files:
                              ; /etc/grid-control.conf, ~/.grid-control.conf and <GCDIR>/default.conf
cmdargs       = -G -c         ; Automatically added command line arguments - default: empty
                              ; Here, -G -c enables the GUI and continuous mode

[jobs]
jobs          = 27            ; Maximum number of jobs (truncated to module maximum)
                              ; Default is taken from module maximum
in flight     = 10            ; Maximum number of concurrently submitted jobs - default: no limit
in queue      = -1            ; Maximum number of queued jobs - default: no limit
max retry     = 4             ; Number of resubmission attempts for failed jobs - default: no limit
continuous    = True          ; Enable continuous mode - default: False
action        = check submit  ; Specify the actions and the order in which grid-control should perform them
                              ; Default: check, retrieve, submit

cpus          = 1             ; Requested number of cpus per node - default: 1
memory        = 512           ; Requested memory in MB - default: 512
                              ; NAF jobs need 2000 MB !
wall time     = 10:00:00      ; Requested wall time in format hh[:mm[:ss]]
                              ; also used for checking the proxy lifetime
cpu time      = 10:00         ; Requested cpu time in format hh[:mm[:ss]] - default: wall time

queue timeout = 2:00:00       ; Resubmit jobs after staying some time in initial state - default: off
node timeout  = 0:10:00       ; Cancel job after some time on worker node - default: off
monitor       = dashboard     ; Monitoring of jobs (can be combined like: "dashboard,scripts")
                              ;    dashboard - use dashboard
                              ;    scripts   - call scripts (see [events] section) [default]
shuffle       = True          ; Submit jobs in random order - default: False
nseeds        = 20            ; Number of random seeds to generate - default: 10
seeds         = 32 51 346 234 ; Random seeds used in the job via @SEED_j@
                              ; @SEED_0@ = 32, 33, 34, ... for first, second, third job
                              ; @SEED_1@ = 51, 52, 53, ... for first, second, third job
                              ; Default: Generate <nseeds> random seeds
selected      = var:KEY=VALUE ; Apply general job selector - default: none
kick offender = 4             ; Threshold for dropping jobs causing status retrieval errors - default: 10

[events]
silent        = True          ; Do not show output of event scripts - default: true
; There are many variables set for these scripts according to the
; environment variables the job will have during execution - they are prefixed "GC_"
; note: the events are only evaluated with "monitor = scripts" in [jobs] section
; In addition to the normal job variables, the following variables are always defined:
;   CFGFILE = absolute path to config file
;   WORKDIR = absolute path to working directory
on submit     = on_submit.sh @CFGFILE@ @JOBNUM@
; To next event has access to the variable "STATUS":
on status     = on_status.sh @NICK@ @STATUS@
; The following event has the additional variable RETCODE (= return code of job)
on output     = on_output.sh @WORKDIR@ @RETCODE@
; For the following event only job invariant constants and the
; variable NJOBS (= number of jobs) are available
on finish     = on_finish.sh @TASK_ID@ @NJOBS@

[dashboard]
application   = Herwigpp      ; Defines application name - default: shellscript
task          = production    ; Task type reported to the dashboard - default: analysis
task name     = @TASK_ID@     ; Taskname in dashboard - default: @TASK_ID@_@NICK@
                              ; ("_" are stripped away from the edges)

; ==============================================================================
; Backend options
; ==============================================================================

[local]
proxy         = TrivialProxy  ; Available options: [TrivialProxy], VomsProxy
check storage = False         ; Check storage requirements before submission - default: True
                              ;   False: Submit jobs even with empty storage requirements
wms           = PBS           ; Select local wms: PBS, LSF, SGE, SLURM, Host - default: best guess
sites         = -node003 nd01 ; Whitelist / Blacklist nodes (prefix "-")
queue         = short long    ; User specified list of local queues
broker        = DummyBroker   ; Available options: [DummyBroker], SimpleBroker
delay output  = False         ; Delay job output / error streaming to end of job - default: False

sandbox path  = %(dir)s/sbox  ; Path to sandboxes - default: $WORKDIR/sandbox
scratch path  = /tmp          ; Override scratch directory on nodes - default: determined by job
server        = servername    ; Name of batch server - default: None
group         = cmsqcd        ; Select local fairshare group

[PBS]
attribute     = value1        ; Forward options to batch system

[grid]
proxy         = VomsProxy     ; Available options: TrivialProxy, [VomsProxy]
wms           = GliteWMS      ; Available options: [GliteWMS], Glite, LCG
sites         = -ce.grid.net  ; Whitelist / Blacklist sites (prefix "-")
vo            = cms           ; Select vo - default: provided by proxy

[proxy]
ignore warnings   = True      ; Ignore problems with the proxy as long as all
                              ; necessary information is provided - default: False

[glite-wms]
use delegate  = False         ; Use delegation proxy for job submissin - default: True
ce            = ce1.gridka.de ; Select destination CE
;config = docs/glite_wms_ALL.conf       ; GliteWMS backend specific configuration (WMS, ...)

; ==============================================================================
; Storage options
; ==============================================================================

[storage]
; se path specifies the default location(s) used to transfer "se input files" at the
; beginning of the job and "se output files" at the end of the job
; Currently supported protocols: gsiftp srm rfio dir
se path       = gsiftp://ekp-lcg-se.physik.uni-karlsruhe.de//wlcg/data/users/cms/my_username
;se path      = srm://dcache-se-cms.desy.de:8443/pnfs/desy.de/cms/tier2/store/user/
;se path      = srm://dcache-se-cms.desy.de:8443//srm/managerv2?SFN=/pnfs/desy.de/cms/tier2/store/user/
;se path      = rfio:///castor/cern.ch/user/x/username
;se path      = dir:///absolute/path/to/directory

se min size       = -1                  ; Job fails if any output file is smaller than se min size
se output path    = dir:///storage/     ; Location to save se output files - default: <se path>
se output files   = out.root            ; Specifies the files to be transfered after the job has finished
se output pattern = job_@MY_JOBID@_@X@  ; This pattern is applied to the se output file
                                        ; to get the destination filename
                                        ; Default: @NICK@job_@MY_JOBID@_@X@
                                        ; @X@         : Marks the spot of the original filename
                                        ; @XBASE@     : Original filename without extension
                                        ; @XEXT@      : Only the extension of the original filename
                                        ; @MY_JOBID@  : Continous job number 0..max
                                        ; @NICK@      : Nickname of dataset for CMSSW jobs
                                        ; @CONF@      : Name of config file (without extension .conf)
                                        ; @DATE@      : Current date eg. 2009-06-24
                                        ; @TIMESTAMP@ : Current timestamp eg. 1245855548
                                        ; @RANDOM@    : Random number between 0 and 900000000
                                        ; This is just the list of the most important substituions available
                                        ; A complete list is available via the --help-vars option
                                        ; The variables can also be surounded by "__" instead of "@"

se input path     = dir:///storage/     ; Location with se input files - default: <se path>
se input files    = file                ; Specifies the files to be transfered before the job starts
se input pattern  = @X@                 ; This pattern is applied to the se input file
                                        ; to get the source filename. Same rules as output pattern
                                        ; Default: @X@
se input timeout = 00:30                ; timeout for the transfer of se input files to the worker node
                                        ; (includes runtime, if 'SE runtime' is True). Default: 0:10 (10 minutes)
se output timeout = 2:00                ; timeout for the transfer of se output files. Default: 1:00 (1 hour)

; During the duration of the job both the available and used space is monitored
; The following entries specify thresholds (in mb) which cause the job to abort
; Landing zone is the directory the job initially starts in
landing zone space used = 100           ; Maximum amount of space used by the job
                                        ; Default: 100 mb
landing zone space left = 50            ; Minimum amout of disk space available
                                        ; Default: 1 mb
; One of the first orders of business for each job is to find a large
; scratch space which will be used as working directory of the job
; If the landing zone ITSELF is the scratch space, the scratch thresholds apply
scratch space used = 5000               ; Maximum amount of space used by the job
                                        ; Default: 5000 mb
scratch space left = 1000               ; Minimum amout of disk space available
                                        ; Default: 1 mb

; ==============================================================================
; User Module
; ==============================================================================

[UserMod]
send executable = False                 ; Put executable into the input sandbox - default: True
executable   = default.sh               ; Name of the script / application
arguments    = param1 param2 param3     ; Parameters for the user application
                                        ; Known variables in the form @VAR@ will be replaced.
                                        ; A complete list is available via the --help-vars option
input files  = input.txt config.sh      ; Input files send together with the job
                                        ; Only for small files - send large files via SE!
subst files  = config.sh                ; These input files will be subjected to variable substituion
                                        ; A complete list is available via the --help-vars option
output files = output.gz                ; Output files retrived together with the job
                                        ; Only for small files - send large files via SE!
depends      = CMSSW                    ; Depends on certain environments eg. Herwig++ job which needs
                                        ; CMSSW. Possible values: CMSSW, gLite
                                        ; The corresponding files are share/env.cmssw.sh, share/env.glite.sh
constants    = CONST1 CONST2            ; Define additional constants used in variable substitution:
CONST1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

; ==============================================================================
; ROOT Module
; ==============================================================================
[ROOTMod]
; Uses exactly the same options as UserMod
root path    = /opt/root                ; Path to ROOT installation


; ==============================================================================
; CMSSW Module
; ==============================================================================

[CMSSW]
project area     = %(dir)s/CMSSW_3_1_0  ; Path to an existing CMSSW project area used for running the jobs
;scram project   = CMSSW CMSSW_3_1_0    ; Used to create a vanilla CMSSW project, eg. for production
;scram arch       = slc4_ia32_gcc345    ; Select scram architecture
                                        ; When given a project area, the default arch is taken from the project
                                        ; Has to be specified when using scram project
cmssw dir        = /path/for/VO_CMS_SW_DIR ; Path to manually add cmssw dir to search path

; Path to CMSSW config file 
config file      = %(project area)s/src/Test/Analysis/cmssw-grid.py
prepare config   = True                 ; Append fragment for CMSSW all config files - default: False
instrumentation fragment = fragment.py  ; Specifies path to fragment - default: <...>/fragmentForCMSSW.py

software requirements    = True         ; Write CMSSW version into job requirements - default: True
gzip output      = True                 ; Gzip the output of the cmsRun command - default: True
se runtime       = True                 ; Send CMSSW runtime via SE instead of sending it together with the job - default: False
se runtime force = True                 ; Force to overwrite existing se runtimes - default: True

; Comment out the variable [jobs] jobs in order to run over all events of the dataset
; Specifiy one dataset 
;dataset = /WmunuJets_pt_80_120/CMSSW_1_3_1-Spring07-1243/GEN-SIM-DIGI-RECO
; Or several by starting with an empty line
dataset =
	/WmunuJets_pt_80_120/CMSSW_1_3_1-Spring07-1243/GEN-SIM-DIGI-RECO#2c1efdb8-d9ba-46d4-b067-72c3d8b19abf
	QCD : /QCD_Pt_470_600/CMSSW_1_5_2-CSA07-2096/GEN-SIM-DIGI-RECO@cms_dbs_prod_local_09
	/CSA07JetMET/CMSSW_1_6_7-CSA07-Tier0-A1-Gumbo/RECO
	Zmm : DBS:/Zmumu/Summer08_IDEAL_V9_v1/GEN-SIM-RAW
;	Nick1 : list:/path/to/local/dbsfile
;	Nick2 : file:/pnfs/to/file|1200
; dataset syntax:
;     [<nickname> : [<protocol> :]] <dataset specifier>
;     Syntax for the dataset specifier depends on the protocol:
;          dbs : <dataset path>[@<instance>][#<block>]
;         list : <path to list of data files>[@<forced prefix>][%<selected dataset>[#<selected block>]]
;                The list provider allows to select single datasets / blocks from a file
;         file : <path to data file>|<number of events>[@SE1,SE2]

; Select default dataset protocol
; Available: [DBSApiv2] (=dbs) FileProvider (=file) ListProvider (=list)
dataset provider = DBSApiv2
dataset refresh  = 4:00       ; Refresh dataset information every 4h
dataset storage check = False ; Check storage requirements before submission - default: True
                              ;   False: Submit jobs even with empty storage requirements

dataset splitter = EventBoundarySplitter ; Available options:
                                         ; EventBoundarySplitter: splits on block boundaries and slices each block into
                                         ;                        jobs with "events per job" events per job
                                         ;                        (Default for CMSSW jobs)
                                         ; HybridSplitter:        hybrid of EventBoundarySplitter and FileBoundarySplitter,
                                         ;                        splits on file and block boundaries and tries to use
                                         ;                        approximately "events per job" events per job
                                         ; FileBoundarySplitter:  splits on file and block boundaries
                                         ;                        and interpret "files per job" as files per job
                                         ;                        (Default for UserMod jobs)
                                         ; BlockBoundarySplitter: split along block boundaries

events per job   = 5000                  ; Set granularity of dataset splitter
                                         ; Without datasets, this sets the variable MAX_EVENTS
files per job    = 5                     ; Set granularity of dataset splitter

lumi filter      = 132440:1-132440:401,  ; apply global luminosity block filter for all DBS datasets
                   132442-132447,        ; Syntax: <RUN>[:<LUMI>]-[<RUN>[:<LUMI>]], ...
                   132473, 132652,       ; (open ended ranges possible)
                   /path/to/json_file    ; alternative way to specify lumi sections is via json file
                   json_fileY|-12345     ; get infos from json file up to run 12345

; Select files to be included in the CMSSW runtime
; Default: -.* -config lib module */data *.xml *.sql *.cf[if] *.py
area files       = -.* -config lib module */data *.xml *.sql *.cf[if] *.py

prolog executable = default.sh          ; List of scripts / applications to execute before cmsRun
                                        ; in the prepared CMSSW area
prolog arguments  = @DATASETNICKNAME@   ; Arguments to be supplied to the prolog scripts
epilog executable = afterjob.sh         ; List of scripts / applications to execute after cmsRun
                                        ; in the prepared CMSSW area
epilog arguments  = @VAR1@ -Lsv         ; Arguments to be supplied to the epilog scripts

arguments        = param1 param2 param3 ; Arguments supplied to cmsRun
;input files     =                      ; Input files send together with the job
                                        ; Only for small files - send large files via SE!
;subst files     =                      ; These input files will be subjected to variable substituion
                                        ; A complete list is available via the --help-vars option
                                        ; Default: the CMSSW config file
;output files    =                      ; Output files retrived together with the job
                                        ; Only for small files - send large files via SE!
constants    = CONST1 CONST2            ; Define additional constants used in variable substitution:
CONST1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

[constants]
; Define additional constants (always uppercase) to allow usage in variable substitution:
; These variables are overridden by the constants specified in the module section
; To define constants with different cases, use "constants = ..." in the section of the module
const1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

; The dataset section contains general options for dataset providers
[dataset]
sites               = -gridka.de        ; White/Blacklist for storage location of dataset based jobs
remove empty blocks = False             ; Remove empty blocks from processing - default: true
remove empty files  = False             ; Remove empty files from processing - default: true
nickname source     = NickNameProducer  ; Name of module to produce nickname depending on dataset name
                                        ; and currently specified nickname - default: SimpleNickNameProducer

limit events        = 10000             ; Stop processing after 10000 events - default: -1
ignore files        = /path/file1       ; List of file names to ignore during processing
                      /path/file2       ; (broken files from dbs)
                      /path/file3
resync interactive  = True              ; Switch for interactive resync mode - default: true
resync mode new     = ignore            ; Resync mode for new files - [append], ignore
resync mode removed = replace           ; Resync mode for removed files - disable, [append], replace, ignore
resync mode expand  = replace           ; Resync mode for expanded files - disable, [append], replace, ignore
resync mode shrink  = replace           ; Resync mode for shrunken files - disable, [append], replace, ignore

resync preserve     = False             ; Preserve affected jobs, which stay the same after resync - default: true
resync reorder      = True              ; Reassign job slots of disabled jobs - default: false

; DBSApiv2
; ------------
; Options specific to DBSApiv2 provider:
;   Select default dbs instance for DBSApiv2 datasets
;   Default: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet (Global CMSSW production DBS server instance)
dbs instance         = http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet
only valid           = True              ; Retrieve only data with status "VALID"
use phedex           = False             ; Switch for site info retrieval via DBS / phedex
phedex sites         = -T0_CH_CERN       ; Black/Whitelist with siteDB names for dataset locations
only complete sites  = True              ; Retrieve only complete dataset blocks

; ScanProvider
; ------------
; Options specific to ScanProvider:
dataset name pattern = Data_@TASK_ID@   ; Dataset naming scheme - default: dataset key
block name pattern   = @BLOCK_KEY@      ; Block naming scheme - default: dataset key

dataset hash keys    = VAR1 VAR2 VAR3   ; List of variables to determine dataset boundaries
block hash keys      = VARA VARB VARC   ; List of variables to determine block boundaries

dataset guard override = VAR1 VAR2      ; Override dataset boundary security measures
block guard override   = VARA VARB      ; Override block boundary security measures

; Available scanners: 
; * OutputDirsFromConfig                  (<> => <OUTPUTDIR>)
source config = /path/to/config.conf    ; Path to external config file
source job selector   = var:KEY=VALUE   ; Selects which output directories are retrieved - default: None
; * OutputDirsFromWork                    (<> => <OUTPUTDIR>)
source directory      = /path/to/output ; Path to root of output directories
; * FilesFromJobInfo                      (<OUTPUTDIR> => <FILE>)
; * FilesFromLS                           (<> => <FILE>)
source directory      = /storage/skim   ; Path to dataset files
; * FilesFromDataProvider               ; (<> => <FILE>)
source dataset path   = dbs://...       ; Take files from dataprovider (as flat list with metadata)
; * MetadataFromModule                    (<FILE> => <FILE>)
ignore module vars    = VAR1 VAR2 VAR2  ; List of ignored module variables - default: <common gc variables>
; * DetermineEvents                       (<FILE> => <FILE>)
events command        = /path/to/getEv  ; Program which returns number of events in file - default: None
events key            = NUM_EVENTS      ; Variable which contains number of events
events default        = 1000            ; Number of events to return in case other methods fail - default: -1
; * MatchOnFilename                       (<FILE> => <FILE>)
filename filter       = *.root *.dat    ; List of filename patterns to match against filenames
; * MatchDelimeter                        (<FILE> => <FILE>)
delimeter match       = _:5             ; Match filenames with specified number (5) of delimeters (_) - default: None
delimeter dataset key = _:0:4           ; Substring of filename to be used to determine dataset / block
delimeter block key   = x:7             ; Format: <delim>[:<start>[:<end>]] - default: None
; * AddFilePrefix                         (<FILE> => <FILE>)
filename prefix       = file://         ; Add a prefix to each entry

; * ObjectsFromCMSSW                      (<OUTPUTDIR> => <OUTPUTDIR>)
include parent infos  = True            ; Parse parent information - default: False
merge config infos    = True            ; Merge config files according to their hash value
; * MetadataFromCMSSW                     (<FILE> => <FILE>)
include config infos  = True            ; Include CMSSW config infos - default: False
; * SEListFromPath                        (<FILE> => <FILE>)
; * LFNFromPath                           (<FILE> => <FILE>)
lfn marker            = /data/          ; Part of path to determine lfn start - default: /store/
; * FilterEDMFiles                      ; (<FILE> => <FILE>)

; List of modules to schedule on "file bus"
scanner = OutputDirsFromConfig FilesFromJobInfo DetermineEvents MatchDelimeter

; Options for a particular dataset with nickname TESTNICK can be specified with:
[dataset TESTNICK]
sites               = -fnal.gov         ; White/Blacklist for storage location of dataset based jobs

; ==============================================================================
; CMSSW Advanced Module
; ==============================================================================
[CMSSW_Advanced]
; Uses exactly the same options as CMSSW
nickname constants  = GLOBALTAG ACTIVE  ; Name of nickname specific variables
ACTIVE              = 'Tracks'          ; Simple form for nickname variable - single value for all nicknames
GLOBALTAG           =  START3X_V26::All ; Default value in case no nickname matches
          2010APRv1 => GR10_P_V5::All   ;   <regular expression for nickname> => <variable value>
          2010APRv2 => GR10_P_V6::All

nickname config     =  skim_MC.py       ; Default CMSSW config file is skim_MC.py
          2010APRv1 => skim_C10.py      ;   <regular expression for nickname> => <config file>
          2010APRv2 => skim_C10_36x.py

nickname lumi filter =                  ; Default luminosity filter
               muPD => 135500-136500    ;   <regular expression for nickname> => <luminosity filter>
                jmt => 135000-136000,136100:10-137000:530,selection.json

; ==============================================================================
; Parameter Module
;   The parameter module allows to run jobs from another module with
;   different parameters.
; ==============================================================================

[ParaMod]
module           = CMSSW                ; Available options: CMSSW, UserMod
jobs             = 4                    ; In case the selected module doesn't specify the maximum number
                                        ; of jobs, set the basic number of jobs (to be multiplied by
                                        ; parameter space size) - default: 1

; ------------------------
; SimpleParaMod
;   The following is specific to SimpleParaMod:
;   SimpleParaMod provides the job with a single parameter
; ------------------------

parameter name   = MYTESTPARAM          ; Specify name of parameter - default: PARAMETER
                                        ; Parameter is set as an environment variable
                                        ; and is useable in subst files via eg. @MYTESTPARAM@
parameter values = 23 42 123            ; Specify parameter values
                                        ; In conjunction with jobs = 4 above this results in
                                        ; 4 x 3 = 12 jobs to be submitted in case the module
                                        ; itself does not specify the maximum number of jobs.
                                        ; If the module does specify the max #jobs
                                        ; (CMSSW with dataset - eg. 6) this would give 6 x 3 = 18 jobs

; ------------------------
; LinkedParaMod
;   The following is specific to LinkedParaMod:
;   LinkedParaMod provides the job with several parameters at once
; ------------------------

parameter name   = CUTLOW:CUTHIGH:XSEC  ; Specify names of parameters - default: PARAMETER
                                        ; The parameters are set as environment variables
                                        ; and are useable in subst files via eg. @XSEC@
parameter values =                      ; Specify parameter values
    20 :  40 : 1.342                    ; In conjunction with jobs = 4 above this results in
    40 : 100 : 2.124                    ; 4 x 5 = 20 jobs to be submitted in case the module
   200 : 300 : 3.134                    ; itself does not specify the maximum number of jobs.
   300 : 600 : 1.235                    ; If the module does specify the max #jobs
   700 : 800 : 0.942                    ; (CMSSW with dataset - eg. 6) this would give 6 x 5 = 30 jobs
                                        ; The first job would get CUTLOW=20, CUTHIGH=40, XSEC=1.342
                                        ; The last job would get CUTLOW=700, CUTHIGH=800, XSEC=0.942

; ------------------------
; FileParaMod
;   The following is specific to FileParaMod:
;   FileParaMod provides the job with several parameters at once from a csv file
; ------------------------

parameter source = parameters.csv       ; Specify the file the parameters are taken from.
                                        ; The header specifies the variable names
parameter source dialect = excel-tab    ; Default is to guess the csv format from the file itself
                                        ; Available options: [sniffed], excel, excel-tab

; ------------------------
; MultiParaMod
;   builds all possible combinations of parameters and parameter tuples.
;   This module is similar to the LinkedParaMod and ParaMod itself.
; ------------------------

parameters  = spam (ham, eggs)    ; Specify a parameter and two linked
                                  ; parameters
(ham, eggs) = (A, a) (B, b)       ; Assign values to the linked parameters
spam        = range(1,3)          ; Assign values to the single parameter
spam type   = expr                ; How to process the right hand side of config options:
                                  ;    words   : Split on whitespace [default]
                                  ;    lines   : Split on newlines
                                  ;    expr    : Evaluate as python expression
                                  ;    binning : Combine input into bins
                                  ;              Example: 1 2 3 4 => (1, 2) (2, 3) (3, 4)

; This gives [spam, ham, eggs]: [0,A,a], [0,B,b], [1,A,a] and [1,B,b] as
; parameter combinations

; In more complex situations which are outside the scope of either
; SimpleParaMod, FileParaMod, LinkedParaMod or MultiParaMod: => derive own module from ParaMod (very easy)
